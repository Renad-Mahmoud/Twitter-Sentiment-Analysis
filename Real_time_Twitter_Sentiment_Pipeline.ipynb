{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3b9a32da16df479dbee8c8dd185fa033": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_38c9a7b7963b4626a56c51955d3716fc",
              "IPY_MODEL_59085f9527fe4c6894a3ea25f78c6364",
              "IPY_MODEL_94669697af324669aa4a4200220d8be5"
            ],
            "layout": "IPY_MODEL_84d22834cba74f54a6f7f18b6cae2703"
          }
        },
        "38c9a7b7963b4626a56c51955d3716fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f826638327c94d82b32562cea24bbad0",
            "placeholder": "​",
            "style": "IPY_MODEL_87d9733519af4e15b703c0090fb194cd",
            "value": "Processing tweets: 100%"
          }
        },
        "59085f9527fe4c6894a3ea25f78c6364": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65161149b6ff4384b68cd2d1430f1196",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4a9c8bdee6414845990e4d0004d40e39",
            "value": 30
          }
        },
        "94669697af324669aa4a4200220d8be5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f190dab78908451ba7cca39f178920d8",
            "placeholder": "​",
            "style": "IPY_MODEL_f3eff09ce8554957957d4a1911076cb6",
            "value": " 30/30 [00:19&lt;00:00,  2.03it/s]"
          }
        },
        "84d22834cba74f54a6f7f18b6cae2703": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f826638327c94d82b32562cea24bbad0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87d9733519af4e15b703c0090fb194cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65161149b6ff4384b68cd2d1430f1196": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a9c8bdee6414845990e4d0004d40e39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f190dab78908451ba7cca39f178920d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3eff09ce8554957957d4a1911076cb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "from tqdm.notebook import tqdm\n",
        "import tweepy\n",
        "from google.colab import drive\n",
        "import getpass\n",
        "import threading\n",
        "import queue"
      ],
      "metadata": {
        "id": "T9aM-AOfPtdz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to access saved model and store results\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6j2lOJEjn_B",
        "outputId": "4affab33-aa91-4eb7-e00b-0b74afff8be4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths\n",
        "MODEL_PATH = '/content/drive/MyDrive/Grad_project/bert_sentiment_model.zip (Unzipped Files)'\n",
        "RESULTS_PATH = '/content/drive/MyDrive/Grad_project/twitter_results'"
      ],
      "metadata": {
        "id": "YrJ3xIerjrRc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure results directory exists\n",
        "if not os.path.exists(RESULTS_PATH):\n",
        "    os.makedirs(RESULTS_PATH, exist_ok=True)"
      ],
      "metadata": {
        "id": "aEHk4Qh6j14Q"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class to handle Twitter API operations\n",
        "class TwitterAPI:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize Twitter API client after getting credentials from user\"\"\"\n",
        "        # Get Twitter API credentials securely\n",
        "        print(\"Please enter your Twitter API credentials:\")\n",
        "        consumer_key = getpass.getpass(\"Consumer Key (API Key): \")\n",
        "        consumer_secret = getpass.getpass(\"Consumer Secret (API Secret): \")\n",
        "        access_token = getpass.getpass(\"Access Token: \")\n",
        "        access_token_secret = getpass.getpass(\"Access Token Secret: \")\n",
        "        bearer_token = getpass.getpass(\"Bearer Token: \")\n",
        "\n",
        "        # Add option to check rate limits before proceeding\n",
        "        print(\"\\nIMPORTANT: Free Twitter API has strict rate limits.\")\n",
        "        choice = input(\"Do you want to check your current rate limit status before proceeding? (y/n): \").lower()\n",
        "\n",
        "        # Authenticate with Twitter API v2\n",
        "        self.client = tweepy.Client(\n",
        "            bearer_token=bearer_token,\n",
        "            consumer_key=consumer_key,\n",
        "            consumer_secret=consumer_secret,\n",
        "            access_token=access_token,\n",
        "            access_token_secret=access_token_secret,\n",
        "            wait_on_rate_limit=True  # Automatically wait when rate limit is reached\n",
        "        )\n",
        "\n",
        "        # Check rate limits if requested\n",
        "        if choice == 'y':\n",
        "            try:\n",
        "                # Get rate limit status for search endpoint\n",
        "                rate_limit = self.client.get_recent_tweets_count(\"test\", granularity=\"day\")\n",
        "                print(\"\\nRate limit information:\")\n",
        "                if hasattr(rate_limit, 'meta') and 'sent' in rate_limit.meta:\n",
        "                    print(f\"Request cost: {rate_limit.meta['sent']}\")\n",
        "                if hasattr(rate_limit, '_headers') and 'x-rate-limit-remaining' in rate_limit._headers:\n",
        "                    print(f\"Remaining requests: {rate_limit._headers['x-rate-limit-remaining']}\")\n",
        "                    print(f\"Rate limit resets in: {rate_limit._headers['x-rate-limit-reset']} seconds\")\n",
        "                else:\n",
        "                    print(\"Could not retrieve detailed rate limit information, but connection successful.\")\n",
        "            except tweepy.TweepyException as e:\n",
        "                print(f\"\\nRate limit check failed: {e}\")\n",
        "                print(\"Proceeding anyway - the code will pause if limits are reached.\")\n",
        "        print(\"Twitter API authentication successful!\")\n",
        "\n",
        "    def search_tweets(self, query, max_results=100, lang='en', tweet_queue=None):\n",
        "        \"\"\"\n",
        "        Search for tweets matching the query and handle pagination and rate limits\n",
        "\n",
        "        Args:\n",
        "            query (str): Search query for Twitter\n",
        "            max_results (int): Maximum number of tweets to retrieve\n",
        "            lang (str): Language filter code (e.g., 'en' for English)\n",
        "            tweet_queue (Queue): Queue to store retrieved tweets for processing\n",
        "\n",
        "        Returns:\n",
        "            list: List of fetched tweets if tweet_queue is None, otherwise None\n",
        "        \"\"\"\n",
        "        all_tweets = []\n",
        "        total_fetched = 0\n",
        "        next_token = None\n",
        "\n",
        "        # Twitter API requires batch_size to be at least 10\n",
        "        batch_size = min(100, max(10, max_results))  # Between 10 and 100\n",
        "\n",
        "        # Add language filter to query if specified\n",
        "        if lang:\n",
        "            query = f\"{query} lang:{lang}\"\n",
        "\n",
        "        # Fields we want to retrieve\n",
        "        tweet_fields = ['created_at', 'public_metrics', 'author_id', 'lang']\n",
        "\n",
        "        try:\n",
        "            while total_fetched < max_results:\n",
        "                # Calculate remaining tweets to fetch\n",
        "                remaining = min(batch_size, max_results - total_fetched)\n",
        "\n",
        "                # Ensure remaining is at least 10 for API requirement\n",
        "                if remaining < 10:\n",
        "                    if total_fetched >= max_results:\n",
        "                        break  # We already have enough tweets\n",
        "                    remaining = min(10, max_results - total_fetched)\n",
        "\n",
        "                print(f\"Fetching batch of {remaining} tweets (total: {total_fetched}/{max_results})...\")\n",
        "\n",
        "                # Search tweets with pagination\n",
        "                response = self.client.search_recent_tweets(\n",
        "                    query=query,\n",
        "                    max_results=remaining,\n",
        "                    tweet_fields=tweet_fields,\n",
        "                    next_token=next_token\n",
        "                )\n",
        "\n",
        "                # If no tweets found, break the loop\n",
        "                if not response.data:\n",
        "                    print(\"No tweets found.\")\n",
        "                    break\n",
        "\n",
        "                # Process each tweet\n",
        "                batch_tweets = []\n",
        "                for tweet in response.data:\n",
        "                    # Skip non-English tweets if English filter is set\n",
        "                    if lang == 'en' and tweet.lang != 'en':\n",
        "                        continue\n",
        "\n",
        "                    tweet_data = {\n",
        "                        'id': tweet.id,\n",
        "                        'text': tweet.text,\n",
        "                        'created_at': tweet.created_at,\n",
        "                        'author_id': tweet.author_id,\n",
        "                        'retweet_count': tweet.public_metrics['retweet_count'],\n",
        "                        'reply_count': tweet.public_metrics['reply_count'],\n",
        "                        'like_count': tweet.public_metrics['like_count'],\n",
        "                        'quote_count': tweet.public_metrics['quote_count'],\n",
        "                        'lang': tweet.lang\n",
        "                    }\n",
        "\n",
        "                    # Either add to queue or collect in list\n",
        "                    if tweet_queue:\n",
        "                        tweet_queue.put(tweet_data)\n",
        "                    else:\n",
        "                        batch_tweets.append(tweet_data)\n",
        "\n",
        "                # If not using queue, add batch to all_tweets list\n",
        "                if not tweet_queue:\n",
        "                    all_tweets.extend(batch_tweets)\n",
        "\n",
        "                # Update total and pagination token\n",
        "                total_fetched += len(response.data)\n",
        "\n",
        "                # Check if we've reached the requested number of tweets\n",
        "                if total_fetched >= max_results:\n",
        "                    break\n",
        "\n",
        "                # Check if we have more tweets to fetch via pagination\n",
        "                if 'next_token' in response.meta:\n",
        "                    next_token = response.meta['next_token']\n",
        "                else:\n",
        "                    print(\"No more pages available.\")\n",
        "                    break\n",
        "\n",
        "                # Add a delay to avoid hitting rate limits\n",
        "                print(\"Waiting 3 seconds before next API request...\")\n",
        "                time.sleep(3)\n",
        "\n",
        "        except tweepy.TweepyException as e:\n",
        "            print(f\"Error fetching tweets: {e}\")\n",
        "\n",
        "        # Signal that we're done if using a queue\n",
        "        if tweet_queue:\n",
        "            tweet_queue.put(None)  # Sentinel value to indicate completion\n",
        "            return None\n",
        "\n",
        "        return all_tweets"
      ],
      "metadata": {
        "id": "8IpdBTcwj4s0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class for sentiment analysis using BERT\n",
        "class SentimentAnalyzer:\n",
        "    def __init__(self, model_path):\n",
        "        \"\"\"\n",
        "        Initialize the BERT model for sentiment analysis\n",
        "\n",
        "        Args:\n",
        "            model_path (str): Path to the saved BERT model\n",
        "        \"\"\"\n",
        "        # Check if GPU is available\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Load tokenizer\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "\n",
        "        # Load model\n",
        "        self.model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()  # Set to evaluation mode\n",
        "\n",
        "        # Define sentiment mapping\n",
        "        # IMPORTANT: This mapping should match your BERT model's output classes\n",
        "        self.sentiments = {0: 'Negative', 1: 'Positive', 2: 'Neutral'}\n",
        "\n",
        "        print(\"Sentiment analysis model loaded successfully!\")\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"\n",
        "        Clean and preprocess text data\n",
        "\n",
        "        Args:\n",
        "            text (str): Text to clean\n",
        "\n",
        "        Returns:\n",
        "            str: Cleaned text\n",
        "        \"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            text = str(text)\n",
        "\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove URLs, HTML tags, and mentions\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "        text = re.sub(r'<.*?>', '', text)\n",
        "        text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "        # Remove special characters but keep emoticons\n",
        "        text = re.sub(r'[^\\w\\s:;)(><]', ' ', text)\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def predict_sentiment(self, text, max_length=128):\n",
        "        \"\"\"\n",
        "        Predict sentiment for a given text\n",
        "\n",
        "        Args:\n",
        "            text (str): Text to analyze\n",
        "            max_length (int): Maximum length for tokenization\n",
        "\n",
        "        Returns:\n",
        "            str: Predicted sentiment label\n",
        "        \"\"\"\n",
        "        # Clean the text\n",
        "        cleaned = self.clean_text(text)\n",
        "\n",
        "        # Tokenize the cleaned text\n",
        "        inputs = self.tokenizer(\n",
        "            cleaned,\n",
        "            return_tensors='pt',\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=max_length\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Get prediction from the model\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            prediction = torch.argmax(outputs.logits, dim=1).item()\n",
        "\n",
        "        return self.sentiments[prediction]\n",
        "\n",
        "    def process_tweet_batch(self, tweets):\n",
        "        \"\"\"\n",
        "        Process a batch of tweets and add sentiment predictions\n",
        "\n",
        "        Args:\n",
        "            tweets (list): List of tweet dictionaries\n",
        "\n",
        "        Returns:\n",
        "            list: Tweets with added sentiment predictions\n",
        "        \"\"\"\n",
        "        for tweet in tweets:\n",
        "            tweet['sentiment'] = self.predict_sentiment(tweet['text'])\n",
        "        return tweets\n",
        "\n",
        "    def process_tweets_from_queue(self, tweet_queue, result_queue):\n",
        "        \"\"\"\n",
        "        Process tweets from a queue and add results to result queue\n",
        "\n",
        "        Args:\n",
        "            tweet_queue (Queue): Queue containing tweets to process\n",
        "            result_queue (Queue): Queue to store processed tweets\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            tweet = tweet_queue.get()\n",
        "            if tweet is None:  # Check for sentinel value\n",
        "                result_queue.put(None)  # Signal completion\n",
        "                break\n",
        "\n",
        "            tweet['sentiment'] = self.predict_sentiment(tweet['text'])\n",
        "            result_queue.put(tweet)"
      ],
      "metadata": {
        "id": "7Vl_20Bnj9Kw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class to handle results visualization and storage\n",
        "class ResultsHandler:\n",
        "    def __init__(self, results_path):\n",
        "        \"\"\"\n",
        "        Initialize the results handler\n",
        "\n",
        "        Args:\n",
        "            results_path (str): Path to store results\n",
        "        \"\"\"\n",
        "        self.results_path = results_path\n",
        "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    def save_to_csv(self, tweets, topic):\n",
        "        \"\"\"\n",
        "        Save tweets to CSV file\n",
        "\n",
        "        Args:\n",
        "            tweets (list): List of tweet dictionaries\n",
        "            topic (str): Search topic\n",
        "\n",
        "        Returns:\n",
        "            str: Path to saved CSV file\n",
        "        \"\"\"\n",
        "        # Check if we have any tweets to save\n",
        "        if not tweets:\n",
        "            print(\"No tweets to save to CSV.\")\n",
        "            filename = f\"{self.results_path}/{topic.replace(' ', '_')}_{self.timestamp}_empty.csv\"\n",
        "            # Create an empty DataFrame with expected columns\n",
        "            empty_df = pd.DataFrame(columns=['id', 'text', 'created_at', 'author_id',\n",
        "                                            'retweet_count', 'reply_count', 'like_count',\n",
        "                                            'quote_count', 'lang', 'sentiment'])\n",
        "            empty_df.to_csv(filename, index=False)\n",
        "            return filename\n",
        "\n",
        "        # Convert list to DataFrame\n",
        "        df = pd.DataFrame(tweets)\n",
        "\n",
        "        # Format created_at column\n",
        "        if 'created_at' in df.columns:\n",
        "            df['created_at'] = pd.to_datetime(df['created_at']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "        # Save to CSV\n",
        "        filename = f\"{self.results_path}/{topic.replace(' ', '_')}_{self.timestamp}.csv\"\n",
        "        df.to_csv(filename, index=False)\n",
        "        print(f\"Results saved to {filename}\")\n",
        "\n",
        "        return filename\n",
        "\n",
        "    def generate_visualizations(self, tweets, topic):\n",
        "        \"\"\"\n",
        "        Generate visualizations based on sentiment analysis\n",
        "\n",
        "        Args:\n",
        "            tweets (list): List of tweet dictionaries\n",
        "            topic (str): Search topic\n",
        "\n",
        "        Returns:\n",
        "            str: Path to saved visualizations or None if no tweets\n",
        "        \"\"\"\n",
        "        # Check if we have any tweets to visualize\n",
        "        if not tweets:\n",
        "            print(\"No tweets to visualize.\")\n",
        "            return None\n",
        "\n",
        "        # Convert to DataFrame for easier manipulation\n",
        "        df = pd.DataFrame(tweets)\n",
        "\n",
        "        # Check if 'sentiment' column exists\n",
        "        if 'sentiment' not in df.columns:\n",
        "            print(\"Error: 'sentiment' column not found in data. Visualizations cannot be generated.\")\n",
        "            return None\n",
        "\n",
        "        # Create directory for visualizations\n",
        "        viz_dir = f\"{self.results_path}/{topic.replace(' ', '_')}_{self.timestamp}_viz\"\n",
        "        os.makedirs(viz_dir, exist_ok=True)\n",
        "\n",
        "        # 1. Sentiment distribution pie chart\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sentiment_counts = df['sentiment'].value_counts()\n",
        "        plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=90, colors=sns.color_palette('viridis'))\n",
        "        plt.title(f'Sentiment Distribution for Topic: \"{topic}\"')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{viz_dir}/sentiment_pie.png\")\n",
        "        plt.close()\n",
        "\n",
        "        # 2. Sentiment distribution bar chart\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        # Fix the FutureWarning by properly using hue parameter\n",
        "        ax = plt.axes()\n",
        "        order = ['Positive', 'Neutral', 'Negative']\n",
        "        sns.countplot(x='sentiment', data=df, palette='viridis', order=order, ax=ax)\n",
        "        plt.title(f'Sentiment Count for Topic: \"{topic}\"')\n",
        "        plt.xlabel('Sentiment')\n",
        "        plt.ylabel('Count')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{viz_dir}/sentiment_bar.png\")\n",
        "        plt.close()\n",
        "\n",
        "        # 3. Average engagement metrics by sentiment\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        metrics = ['like_count', 'retweet_count', 'reply_count', 'quote_count']\n",
        "\n",
        "        # Calculate average metrics by sentiment\n",
        "        sentiment_metrics = df.groupby('sentiment')[metrics].mean().reset_index()\n",
        "\n",
        "        # Reshape for plotting\n",
        "        sentiment_metrics_melted = pd.melt(sentiment_metrics, id_vars=['sentiment'], value_vars=metrics,\n",
        "                                          var_name='Metric', value_name='Average Count')\n",
        "\n",
        "        # Create the grouped bar chart\n",
        "        sns.barplot(x='sentiment', y='Average Count', hue='Metric', data=sentiment_metrics_melted, palette='viridis')\n",
        "        plt.title(f'Average Engagement by Sentiment for Topic: \"{topic}\"')\n",
        "        plt.xlabel('Sentiment')\n",
        "        plt.ylabel('Average Count')\n",
        "        plt.legend(title='Engagement Metric')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{viz_dir}/engagement_by_sentiment.png\")\n",
        "        plt.close()\n",
        "\n",
        "        # 4. Language distribution (if non-English tweets are included)\n",
        "        if len(df['lang'].unique()) > 1:\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            lang_counts = df['lang'].value_counts().head(10)  # Show top 10 languages\n",
        "            sns.barplot(x=lang_counts.index, y=lang_counts.values, palette='viridis')\n",
        "            plt.title(f'Top Languages for Topic: \"{topic}\"')\n",
        "            plt.xlabel('Language Code')\n",
        "            plt.ylabel('Count')\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f\"{viz_dir}/language_distribution.png\")\n",
        "            plt.close()\n",
        "\n",
        "        print(f\"Visualizations saved to {viz_dir}\")\n",
        "        return viz_dir"
      ],
      "metadata": {
        "id": "y-Hkfdd9kBXE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main pipeline class\n",
        "class TwitterSentimentPipeline:\n",
        "    def __init__(self, model_path, results_path):\n",
        "        \"\"\"\n",
        "        Initialize the complete Twitter sentiment analysis pipeline\n",
        "\n",
        "        Args:\n",
        "            model_path (str): Path to the sentiment analysis model\n",
        "            results_path (str): Path to store results\n",
        "        \"\"\"\n",
        "        self.twitter_api = TwitterAPI()\n",
        "        self.sentiment_analyzer = SentimentAnalyzer(model_path)\n",
        "        self.results_handler = ResultsHandler(results_path)\n",
        "\n",
        "    def run_streaming_pipeline(self, topic, max_results=100, lang='en'):\n",
        "        \"\"\"\n",
        "        Run the pipeline in streaming mode (process tweets as they come)\n",
        "\n",
        "        Args:\n",
        "            topic (str): Topic to search for\n",
        "            max_results (int): Maximum number of tweets to retrieve\n",
        "            lang (str): Language filter code (e.g., 'en' for English)\n",
        "\n",
        "        Returns:\n",
        "            tuple: (path to CSV file, path to visualizations)\n",
        "        \"\"\"\n",
        "        # Validate max_results\n",
        "        max_results = min(max(max_results, 10), 100)  # Between 10 and 100\n",
        "\n",
        "        lang_filter = f\"(language filter: {lang})\" if lang else \"(no language filter)\"\n",
        "        print(f\"Starting sentiment analysis for topic: '{topic}' {lang_filter} (max {max_results} tweets)\")\n",
        "\n",
        "        # Create queues for tweet processing\n",
        "        tweet_queue = queue.Queue()\n",
        "        result_queue = queue.Queue()\n",
        "\n",
        "        # Start tweet processing thread\n",
        "        processing_thread = threading.Thread(\n",
        "            target=self.sentiment_analyzer.process_tweets_from_queue,\n",
        "            args=(tweet_queue, result_queue)\n",
        "        )\n",
        "        processing_thread.start()\n",
        "\n",
        "        # Start tweet fetching (this will put tweets into the tweet_queue)\n",
        "        self.twitter_api.search_tweets(topic, max_results, lang, tweet_queue)\n",
        "\n",
        "        # Collect processed results\n",
        "        all_results = []\n",
        "        processed_count = 0\n",
        "\n",
        "        # Create progress bar\n",
        "        pbar = tqdm(total=max_results, desc=\"Processing tweets\")\n",
        "\n",
        "        while True:\n",
        "            result = result_queue.get()\n",
        "            if result is None:  # Check for sentinel value\n",
        "                break\n",
        "\n",
        "            all_results.append(result)\n",
        "            processed_count += 1\n",
        "            pbar.update(1)\n",
        "\n",
        "            # Print current result (optional)\n",
        "            print(f\"Tweet: '{result['text'][:50]}...' - Sentiment: {result['sentiment']}\")\n",
        "\n",
        "        pbar.close()\n",
        "        processing_thread.join()\n",
        "\n",
        "        print(f\"Processed {len(all_results)} tweets.\")\n",
        "\n",
        "        # Handle cases with no tweets\n",
        "        if len(all_results) == 0:\n",
        "            print(\"Warning: No tweets were processed. Results may be limited.\")\n",
        "\n",
        "        # Save results to CSV\n",
        "        csv_path = self.results_handler.save_to_csv(all_results, topic)\n",
        "\n",
        "        # Generate visualizations if we have results\n",
        "        viz_path = None\n",
        "        if all_results:\n",
        "            viz_path = self.results_handler.generate_visualizations(all_results, topic)\n",
        "\n",
        "        return csv_path, viz_path"
      ],
      "metadata": {
        "id": "agIim-DFkFUB"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to test the pipeline\n",
        "def test_pipeline():\n",
        "    \"\"\"Test the Twitter sentiment analysis pipeline with user input\"\"\"\n",
        "    # Initialize the pipeline\n",
        "    pipeline = TwitterSentimentPipeline(MODEL_PATH, RESULTS_PATH)\n",
        "\n",
        "    # Get user input\n",
        "    topic = input(\"Enter topic to search for on Twitter: \")\n",
        "\n",
        "    try:\n",
        "        print(\"\\nNOTE: For free API tier, we recommend keeping this number small (10-50)\")\n",
        "        print(\"IMPORTANT: Twitter API requires a minimum of 10 tweets per request\")\n",
        "        max_results = int(input(\"Enter number of tweets to analyze (min 10, max 100): \"))\n",
        "        # Ensure between 10 and 100\n",
        "        max_results = min(max(max_results, 10), 100)\n",
        "    except ValueError:\n",
        "        print(\"Invalid input. Using default of 10 tweets.\")\n",
        "        max_results = 10\n",
        "\n",
        "    # Get language preference\n",
        "    lang_choice = input(\"\\nFilter tweets by language? (y/n): \").lower()\n",
        "    lang = 'en' if lang_choice == 'y' else None\n",
        "\n",
        "    if lang_choice == 'y':\n",
        "        print(\"English language filter applied. Only English tweets will be analyzed.\")\n",
        "    else:\n",
        "        print(\"No language filter applied. Tweets in all languages will be analyzed.\")\n",
        "\n",
        "    print(\"\\nUsing streaming mode to process tweets as they come (better for handling rate limits)\")\n",
        "    print(\"\\nIMPORTANT: If you see 'Rate limit exceeded' warning, don't worry!\")\n",
        "    print(\"The program will automatically wait and continue when Twitter allows more requests.\")\n",
        "    print(\"This is normal for free API tier accounts.\")\n",
        "\n",
        "    try:\n",
        "        # Run streaming pipeline\n",
        "        csv_path, viz_path = pipeline.run_streaming_pipeline(topic, max_results, lang)\n",
        "\n",
        "        if csv_path:\n",
        "            print(\"\\nAnalysis completed!\")\n",
        "            print(f\"Results saved to: {csv_path}\")\n",
        "            if viz_path:\n",
        "                print(f\"Visualizations saved to: {viz_path}\")\n",
        "            else:\n",
        "                print(\"No visualizations were generated (likely no tweets found).\")\n",
        "        else:\n",
        "            print(\"\\nAnalysis failed. Please check the logs above for errors.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred during analysis: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "id": "OKSl_Z-okJOJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the test function if this script is executed directly\n",
        "if __name__ == \"__main__\":\n",
        "    test_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3b9a32da16df479dbee8c8dd185fa033",
            "38c9a7b7963b4626a56c51955d3716fc",
            "59085f9527fe4c6894a3ea25f78c6364",
            "94669697af324669aa4a4200220d8be5",
            "84d22834cba74f54a6f7f18b6cae2703",
            "f826638327c94d82b32562cea24bbad0",
            "87d9733519af4e15b703c0090fb194cd",
            "65161149b6ff4384b68cd2d1430f1196",
            "4a9c8bdee6414845990e4d0004d40e39",
            "f190dab78908451ba7cca39f178920d8",
            "f3eff09ce8554957957d4a1911076cb6"
          ]
        },
        "id": "5qfsiptIkMGB",
        "outputId": "2d57dc46-1537-4532-9dc6-92aad05e1e23"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter your Twitter API credentials:\n",
            "Consumer Key (API Key): ··········\n",
            "Consumer Secret (API Secret): ··········\n",
            "Access Token: ··········\n",
            "Access Token Secret: ··········\n",
            "Bearer Token: ··········\n",
            "\n",
            "IMPORTANT: Free Twitter API has strict rate limits.\n",
            "Do you want to check your current rate limit status before proceeding? (y/n): y\n",
            "\n",
            "Rate limit information:\n",
            "Could not retrieve detailed rate limit information, but connection successful.\n",
            "Twitter API authentication successful!\n",
            "Using device: cpu\n",
            "Sentiment analysis model loaded successfully!\n",
            "Enter topic to search for on Twitter: dell\n",
            "\n",
            "NOTE: For free API tier, we recommend keeping this number small (10-50)\n",
            "IMPORTANT: Twitter API requires a minimum of 10 tweets per request\n",
            "Enter number of tweets to analyze (min 10, max 100): 30\n",
            "\n",
            "Filter tweets by language? (y/n): y\n",
            "English language filter applied. Only English tweets will be analyzed.\n",
            "\n",
            "Using streaming mode to process tweets as they come (better for handling rate limits)\n",
            "\n",
            "IMPORTANT: If you see 'Rate limit exceeded' warning, don't worry!\n",
            "The program will automatically wait and continue when Twitter allows more requests.\n",
            "This is normal for free API tier accounts.\n",
            "Starting sentiment analysis for topic: 'dell' (language filter: en) (max 30 tweets)\n",
            "Fetching batch of 30 tweets (total: 0/30)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing tweets:   0%|          | 0/30 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3b9a32da16df479dbee8c8dd185fa033"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet: '@Val_Gadget Dell any day...' - Sentiment: Neutral\n",
            "Tweet: 'RT @_MLFootball: 🚨🚨THIS IS WILD🚨🚨\n",
            "\n",
            "#TEXANS STAR RE...' - Sentiment: Negative\n",
            "Tweet: 'We're thrilled to inform you that you might be eli...' - Sentiment: Neutral\n",
            "Tweet: 'I wish more articles were as informative as this o...' - Sentiment: Positive\n",
            "Tweet: 'RT @BTC_for_Freedom: Imagine how much Bitcoin Mich...' - Sentiment: Negative\n",
            "Tweet: 'Dell Rolls Out Trusted Device 5.0 with Enhanced Se...' - Sentiment: Positive\n",
            "Tweet: '🔥 The BIGGЕST #Сryрtо #РUMР #Signаl is here! 🚀 Jоi...' - Sentiment: Neutral\n",
            "Tweet: 'Tank Dell with the new ink!\n",
            "\n",
            " https://t.co/5tr3amp...' - Sentiment: Neutral\n",
            "Tweet: 'RT @storagereview: The Dell PowerEdge R7715 is a 2...' - Sentiment: Neutral\n",
            "Tweet: 'RT @Val_Gadget: Looking for a new laptop?\n",
            "\n",
            "Between...' - Sentiment: Neutral\n",
            "Tweet: '@MichaelDell @DellTech I’ve never owned anything b...' - Sentiment: Neutral\n",
            "Tweet: 'Dangerous for wood, it has many ennemies.\n",
            "#hanneto...' - Sentiment: Negative\n",
            "Tweet: '🔥 The BIGGЕST #Сryрtо #РUMР #Signаl is here! 🚀 Jоi...' - Sentiment: Neutral\n",
            "Tweet: 'RT @_MLFootball: 🚨🚨THIS IS WILD🚨🚨\n",
            "\n",
            "#TEXANS STAR RE...' - Sentiment: Negative\n",
            "Tweet: '@patrickbetdavid $BTBT and $DELL lets go!...' - Sentiment: Positive\n",
            "Tweet: 'RT @_MLFootball: 🚨🚨THIS IS WILD🚨🚨\n",
            "\n",
            "#TEXANS STAR RE...' - Sentiment: Negative\n",
            "Tweet: '@MichaelDell @DellTech Congrats, Michael Dell @Del...' - Sentiment: Positive\n",
            "Tweet: '@RealAlexJones caramouche (French: [skaʁamuʃ]) or ...' - Sentiment: Neutral\n",
            "Tweet: 'RT @BurgessMar3991: @MichaelDell @DellTech You nee...' - Sentiment: Negative\n",
            "Tweet: '@MichaelDell @DellTech Dude! You're getting a DELL...' - Sentiment: Positive\n",
            "Tweet: '@ddakinola Dell XPS 9380 \n",
            "Core i5-8365U (8th Gen)\n",
            "...' - Sentiment: Neutral\n",
            "Tweet: '@BanabasSamuel Dell XPS 9380 \n",
            "Core i5-8365U (8th G...' - Sentiment: Neutral\n",
            "Tweet: '@UtdAce8 Dell XPS 9380 \n",
            "Core i5-8365U (8th Gen)\n",
            "8G...' - Sentiment: Neutral\n",
            "Tweet: 'RT @statquants: $DELL Insider Watch: Massive $9.92...' - Sentiment: Neutral\n",
            "Tweet: 'RT @freshkaptain: This is all we need! Give me the...' - Sentiment: Positive\n",
            "Tweet: 'RT @SpidermanNotes: Spider-Man by Gabriele Dell'Ot...' - Sentiment: Neutral\n",
            "Tweet: '@poofy_poof42 WHAT THE FUCK DO U MEAN DELL MAKES L...' - Sentiment: Negative\n",
            "Tweet: '@Val_Gadget Dell...' - Sentiment: Neutral\n",
            "Tweet: 'RT @_MLFootball: 🚨🚨THIS IS WILD🚨🚨\n",
            "\n",
            "#TEXANS STAR RE...' - Sentiment: Negative\n",
            "Tweet: 'RT @VincenzoSelXXX: ‼️ $3.99 ONLY ‼️\n",
            "I’m a human t...' - Sentiment: Negative\n",
            "Processed 30 tweets.\n",
            "Results saved to /content/drive/MyDrive/Grad_project/twitter_results/dell_20250503_235525.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-cbad29735fa6>:91: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.countplot(x='sentiment', data=df, palette='viridis', order=order, ax=ax)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visualizations saved to /content/drive/MyDrive/Grad_project/twitter_results/dell_20250503_235525_viz\n",
            "\n",
            "Analysis completed!\n",
            "Results saved to: /content/drive/MyDrive/Grad_project/twitter_results/dell_20250503_235525.csv\n",
            "Visualizations saved to: /content/drive/MyDrive/Grad_project/twitter_results/dell_20250503_235525_viz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RM9tqNarkpy3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}